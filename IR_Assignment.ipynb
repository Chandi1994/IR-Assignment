{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-Assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxEQTTojpZqvKHWaUbYLG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chandi1994/IR-Assignment/blob/main/IR_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ4XTguKZQxD"
      },
      "source": [
        "# Loading Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3Zhv4xzYtjv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import path as osp\n",
        "from bs4 import BeautifulSoup\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4owgNgYea62g"
      },
      "source": [
        "# Main text file was seperated into three files based on the content\n",
        "\n",
        "file_name = '/content/assignment_data.txt'\n",
        "\n",
        "feedback_file = \"/content/feedback.txt\"\n",
        "research_paper_file = \"/content/research_paper.txt\"\n",
        "twitter_file = \"/content/twitter.txt\"\n",
        "\n",
        "output_location = '/content/output.txt'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWNUt5LkcDQC"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8qQjbN8b_x6"
      },
      "source": [
        "data = {}\n",
        "data['research'] = open(research_paper_file, mode = 'r', encoding='utf-8').read()\n",
        "data['twitter'] = open(twitter_file, mode = 'r', encoding='utf-8').read()\n",
        "data['feedback'] = open(feedback_file, mode = 'r', encoding='utf-8').read()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaPKzWJccI6",
        "outputId": "aa87702e-34b3-4232-debd-87677ea7e545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['research'][:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Neural network models have shown their promising opportunities for multi-task\\nlearning, which focus '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN64HQXDchM1",
        "outputId": "3d30c4bb-4b0f-4246-e6ef-e42808fc4e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['twitter'][:100]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Reminds me of Liberal Immigration Fraudster Monsef avoiding deportation from Canada. #cdnpoli #LPC #'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRLJFxMwck8W",
        "outputId": "cd7b3adf-f754-4800-968f-b934dd90c9cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data['feedback'][:100]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Honestly last seven lectures are good. Lectures are understandable. Lecture slides are very useful t'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syKU1x06ctqG"
      },
      "source": [
        "def write_to_file(text_lines, file_name):\n",
        "    full_path = osp.join(output_location, file_name)\n",
        "    dir_path = osp.dirname(full_path)\n",
        "    if not osp.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        \n",
        "    with open(full_path, 'w') as fp:\n",
        "#         print(text_lines)\n",
        "        for line in text_lines:\n",
        "#             print(line)\n",
        "            fp.write(line + ' ')\n",
        "    print(\"Written to {}\".format(full_path))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YSMkKJ_cybo"
      },
      "source": [
        "# 1. Tokenize data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Skw-GudpcA"
      },
      "source": [
        "#1.1 Tokenize Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqXA9V4xcvqa",
        "outputId": "a38e9d9c-12ac-40cd-af22-8977b4ac7375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For select best tokenizer to tokenize twitter data try with 4 different tokenizers\n",
        "\n",
        "compare_list = ['#cdnpoli', '#LPC #CPCLDR��_', 'https://t.co/ZOZOSe1CqQ']\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "punct_tokenizer = WordPunctTokenizer()\n",
        "punct_tokens = []\n",
        "for sent in compare_list:\n",
        "    print(punct_tokenizer.tokenize(sent))\n",
        "    punct_tokens.append(punct_tokenizer.tokenize(sent))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#', 'cdnpoli']\n",
            "['#', 'LPC', '#', 'CPCLDR', '��', '_']\n",
            "['https', '://', 't', '.', 'co', '/', 'ZOZOSe1CqQ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeQtXkSid48k",
        "outputId": "f3b0d581-3c26-4266-b535-8f29b11df65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "match_tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "match_tokens = []\n",
        "for sent in compare_list:   \n",
        "    print(match_tokenizer.tokenize(sent))\n",
        "    match_tokens.append(match_tokenizer.tokenize(sent))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cdnpoli']\n",
            "['LPC', 'CPCLDR', '_']\n",
            "['https', 't', 'co', 'ZOZOSe1CqQ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9opAX659d5_S",
        "outputId": "4d374e2d-7681-4c68-b75d-90afd449eb8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "space_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
        "space_tokens = []\n",
        "for sent in compare_list:\n",
        "    \n",
        "    print(space_tokenizer.tokenize(sent))\n",
        "    space_tokens.append(space_tokenizer.tokenize(sent))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#cdnpoli']\n",
            "['#LPC', '#CPCLDR��_']\n",
            "['https://t.co/ZOZOSe1CqQ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slAo3cVjd-l0",
        "outputId": "ad7f78d4-ae0d-4fdc-d73a-7d57cb0b816c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = []\n",
        "for sent in compare_list:\n",
        "    print(tweet_tokenizer.tokenize(sent))\n",
        "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#cdnpoli']\n",
            "['#LPC', '#CPCLDR', '�', '�', '_']\n",
            "['https://t.co/ZOZOSe1CqQ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SneuokWeALg",
        "outputId": "6049541e-8cea-470f-a063-bbb87a60b80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "twitter_tokenize = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize(data['twitter'])\n",
        "print(twitter_tokenize)\n",
        "write_to_file(twitter_tokenize, 'tokenize/{}_output.txt'.format('twitter'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#cdnpoli', '#LPC', '#CPCLDR', '�', '�', '_', 'https://t.co/ZOZOSe1CqQ', '#immigration', '#integration', '#canada', 'https://t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'Canada', '.', 'https://t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https://t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https://t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '�', '�', '�', 'Substantially', 'Increase', 'Immigration', 'Numbers', 'https://t.co/nEFw30MRaa', 'https://t.co/cyI867PZRV', 'M', '�', '�', 'me', 'les', '#USA', '=p', 'ays', \"d'immigration\", 'par', 'excellence', 'CONTR', '�', '�', 'LE', 'RIGOUREUSEMENT', \"l'immigration\", 'et', 'acc', '�', '�', 's', '�', '�', 'la', '#GreenCARD', '!', '�', '�', '_', 'https://t.co/IHpVhW2BaG', 'what', 'changes', 'should', 'be', 'made', 'to', \"Canada's\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', '?', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', 'https://t.co/f4utO5A7ZF', \"L'immigration\", 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', '-', 'https://t.co/UiBsEZOqas', 'https://t.co/j77dEvjoiX', 'https://t.co/XXDeIG7Dbu', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', '_', '�', '�', '_', '�', '_', '?', '?', 'From', 'my', 'view', '-', '-', 'immigration', 'out', 'of', 'C', '�', '�', '_', 'https://t.co/YAgwmZ8ECp', 'Dan', 'Murray', 'of', '�', '�', 'Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear-mongering', 'liar', '#racism', '#canada', '#cdnpoli', '#hatecrime', '�', '�', '_', 'https://t.co/kwZ3csvYxM', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', '�', '�', 'son', 'besoin', 'de', 'main', 'd', \"'\", '�', '�', 'uvre', 'https://t.co/kXdfMGTZzN', 'L', '�', '�', '#immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', '#Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/s3hu1OKKIG', \"I've\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'US', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', '#fasttraffic', ',', '#sitetraffic', ',', '#website', ',', '#traffic', 'https://t.co/zRlJ26jnkC', 'Mr', 'Know-all', 'of', 'Canada', 'Immigration', 'https://t.co/wTQK4QDiKI', 'Move', 'to', 'Canada', 'Oh', ',', 'immigration', 'rules', ',', 'you', \"can't\", '...', 'https://t.co/5LIEVHO7A4', '#OnThisDay', 'Annette', 'Toft', 'becomes', \"Canada's\", '2', 'millionth', 'immigrant', 'since', '1945', '.', 'Do', 'you', 'know', 'your', \"family's\", 'immigration', 'st', '�', '�', '_', 'https://t.co/UvRuw8eR1b', '.', 'profiles', \"Canada's\", 'open', 'immigration', 'policies', '&', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', ':', '�', '�', '_', 'https://t.co/4K84EE8Y63', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', ',', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https://t.co/x2IfO0EXI2', 'Immigration', 'for', 'canada', 'without', 'india', ':', 'an', 'compassionate', 'handle', ':', 'deyFy', '\"', '#Jamaican', '#immigrants', '#Canada', 'https://t.co/vcmfYGadR5', '#statistics', '#immigration', '\"', 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', '$', '262M', 'over', 'a', 'decade', 'https://t.co/9i72fRhtij', 'Are', 'people', 'still', 'moving', 'to', '#Canada', '?', '?', '?', 'Oh', \"that's\", 'right', ',', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's\", '�', '�', '_', 'https://t.co/0C5OBfmxLG', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', ',', 'B', '.', 'C', '.', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https://t.co/YXH5W53srO', 'I', 'added', 'a', 'video', 'to', 'a', 'playlist', 'https://t.co/CnEyWN40x3', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa-Free', 'To', 'Canada', 'https://t.co/Ec3XHORO2s', 'https://t.co/RQRr5nebcG', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/DkpuKyWmaK', 'Hes', 'the', 'POS', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', ',', 'among', 'other', 'globalist', 'policies', '.', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', ',', '2016', '.', 'Thoughts', '?', '#visa', '#immigration', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass']\n",
            "Written to /content/output.txt/tokenize/twitter_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1uc2NHbfrEq"
      },
      "source": [
        "# 1.2 Tokenize Student Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcBpXlD3fkgU",
        "outputId": "41d31dae-c895-4d7e-8432-79914a459543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For select best tokenizer to tokenize twitter data try with 4 different tokenizers\n",
        "\n",
        "sentence = '<br />please do recap at class starting it&#039;s better for us.'\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "tokens = text_to_word_sequence(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['br', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '039', 's', 'better', 'for', 'us']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ocamw_XfgJJs",
        "outputId": "988a9734-551f-4390-d2d7-88e0d103aa95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "print(list(tokenize(sentence)))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['br', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', 's', 'better', 'for', 'us']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsH7RZgNgLqi",
        "outputId": "42b948a1-ac1c-4ef1-f4b5-bb2e252cf8e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = TreebankWordTokenizer().tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk9UOkoXg_YT",
        "outputId": "cdb4d50a-e8f3-4429-906b-d474caad8dba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jSbhfsChFEg",
        "outputId": "24de2f64-6d54-42f7-a413-ad49fab41cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "student_feedback = word_tokenize(data['feedback'])\n",
        "print(student_feedback)\n",
        "write_to_file(student_feedback, 'tokenize/{}_output.txt'.format('feedback'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.', '<', 'br', '/', '>', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Thanks', '!', ':', ')', '<', 'br', '/', '>', '``', 'The', 'lectures', 'are', 'good..but', 'a', 'bit', 'speed.A', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'one.So', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame.', \"''\", '<', 'br', '/', '>', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', '.', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', '.', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', '.', '<', 'br', '/', '>', \"''\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', '.', 'It', 'was', 'easy', 'to', 'understand', '.', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', '.', 'Lectures', 'were', 'good', '.', 'understandable', '.', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'OOP', '.', 'Motivated', 'to', 'well', '.', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', '.', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', '.', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', '.', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', '.', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples.lectures', 'were', 'interesting.we', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', '.', 'I', 'satisfy', 'about', 'first', '7', 'lectures', '.', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', '.', 'lectuers', 'are', 'very', 'good', '.', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', '.', 'very', 'helpfull', '.', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'OOP', 'and', 'its', 'concepts', '.', '``', 'lecture', 'slides', ',', 'explanations', 'were', 'very', 'clear', '.', '<', 'br', '/', '>', 'it', '&', '#', '039', ';', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', '.', '<', 'br', '/', '>', 'sometimes', ',', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', '.', '<', 'br', '/', '>', 'overall', 'very', 'good', '!', '!', '!', '<', 'br', '/', '>', \"''\", 'The', 'lectures', 'were', 'good', 'and', 'clear', '.', 'And', 'they', 'weren', '&', '#', '039', ';', 't', 'too', 'fast', '.', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', '&', '#', '039', ';', 't', 'know', 'java', 'before', '.', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'class.it', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', '!', '.', 'thankyou']\n",
            "Written to /content/output.txt/tokenize/feedback_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnf-hBTHixEh"
      },
      "source": [
        "# 1.3 Tokenize Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ_UmH1uivih",
        "outputId": "f8b411e0-83fe-4729-dc60-7416819fee2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For select best tokenizer to tokenize twitter data try with 4 different tokenizers\n",
        "\n",
        "sentence = 'However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the'\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "tokens = text_to_word_sequence(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['however', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'liu', 'et', 'al', '2016c', 'b', 'attempts', 'to', 'divide', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzCjINF4jhxw",
        "outputId": "3add4e45-8205-48c9-8b4d-2ae9dfc745ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gensim.utils import tokenize\n",
        "print(list(tokenize(sentence)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['However', 'most', 'existing', 'work', 'on', 'multi', 'task', 'learning', 'Liu', 'et', 'al', 'c', 'b', 'attempts', 'to', 'divide', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UMZTCZjmU3",
        "outputId": "b7b8d9a8-368b-430a-e2d9-62b662970060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = TreebankWordTokenizer().tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fUg0dsLjpgk",
        "outputId": "7a95bb05-6fa0-42f9-8a7f-b43d4ca6e245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuuwUvj_jsI4",
        "outputId": "df51fd5e-bac1-40cf-ee52-3bba7f53c366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "research_data = word_tokenize(data['research'])\n",
        "print(research_data)\n",
        "write_to_file(research_data, 'tokenize/{}_output.txt'.format('research'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al.', ',', '2016', ';', 'Zhang', 'et', 'al.', ',', '2014', ')', 'to', 'natural', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', 'However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-', '(', 'a', ')', ',', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features', ',', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features', ',', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', 'However', ',', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework', ',', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.']\n",
            "Written to /content/output.txt/tokenize/research_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tGOmvhwkNKg"
      },
      "source": [
        "# 2. Isolated word correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4BmRW63kB0w"
      },
      "source": [
        "def spell_ckecking(tockens):\n",
        "    spell = SpellChecker(distance=2)\n",
        "    mispelled = spell.unknown(tockens)\n",
        "    pairs = []\n",
        "    print(\"Mispelled count: {}\".format(len(mispelled)))\n",
        "    for i, word in enumerate(mispelled):\n",
        "        correction = spell.correction(word)\n",
        "        print('{:2} - \"{}\" is corrected as \"{}\"'.format(i, word, correction))\n",
        "        pairs.append((word, correction))\n",
        "        if i == 5:\n",
        "            break\n",
        "    return pairs"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH0sCrYjlIyC"
      },
      "source": [
        "# 2.1 Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98_LaX7llEyi",
        "outputId": "ee4104d1-4d06-47ae-9cad-a2840b2c740a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_pairs = spell_ckecking(twitter_tokenize)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mispelled count: 78\n",
            " 0 - \"cortiqu\" is corrected as \"cortina\"\n",
            " 1 - \"rigoureusement\" is corrected as \"rigoureusement\"\n",
            " 2 - \"#visa\" is corrected as \"visa\"\n",
            " 3 - \"#canada\" is corrected as \"canada\"\n",
            " 4 - \"https://t.co/wtqk4qdiki\" is corrected as \"https://t.co/wtqk4qdiki\"\n",
            " 5 - \"https://t.co/9i72frhtij\" is corrected as \"https://t.co/9i72frhtij\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcghjq6NmVBp"
      },
      "source": [
        "# 2.2 Student Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkDgcvklmT9t",
        "outputId": "de75a2b1-7fb1-4d17-d999-16dd227fd0d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_pairs = spell_ckecking(student_feedback)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mispelled count: 14\n",
            " 0 - \"''\" is corrected as \"d'\"\n",
            " 1 - \"helpfull\" is corrected as \"helpful\"\n",
            " 2 - \"one.so\" is corrected as \"ones\"\n",
            " 3 - \"interesting.we\" is corrected as \"interesting.we\"\n",
            " 4 - \"examples.lectures\" is corrected as \"examples.lectures\"\n",
            " 5 - \"lectuers\" is corrected as \"lectures\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n8vphp_mlbn"
      },
      "source": [
        "# 2.3 Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-ZaqPAmtOd",
        "outputId": "236ce7b4-9397-44fe-cdd5-8d54e291e32d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_pairs = spell_ckecking(research_data)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mispelled count: 16\n",
            " 0 - \"collobert\" is corrected as \"colbert\"\n",
            " 1 - \"andweston\" is corrected as \"anderton\"\n",
            " 2 - \"2016c\" is corrected as \"2016c\"\n",
            " 3 - \"multi-task\" is corrected as \"multi-track\"\n",
            " 4 - \"task-specific\" is corrected as \"task-specific\"\n",
            " 5 - \"shared-private\" is corrected as \"shared-private\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mcENinMnwcS"
      },
      "source": [
        "# 3. Context Sensitive word correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcXtCFZKn1ZH",
        "outputId": "b3a926fc-564f-4597-9ffb-169a3b098a15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "word_length = 2\n",
        "prefix_length = 7\n",
        "sym_spell = SymSpell(word_length, prefix_length)\n",
        "print(\"Corpus file not found\") if not sym_spell.create_dictionary(\"/content/assignment_data.txt\") else print(\"Success!\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V77UWMT9o8_A"
      },
      "source": [
        "preview = 10\n",
        "def correct_tokenized_text(words):\n",
        "    corr_count = 0\n",
        "    corrected_words = []\n",
        "    for i, word in enumerate(words[:-word_length+1]):\n",
        "        word_set = [words[i+j] for j in range(word_length)]\n",
        "        _input = ' '.join(word_set)\n",
        "        result = sym_spell.word_segmentation(_input)\n",
        "        correction = result.corrected_string\n",
        "        if correction.lower() != _input.lower() and preview < corr_count:\n",
        "            corr_count += 1\n",
        "            print('\"{}\" is corrected as \"{}\"'.format(_input, correction))\n",
        "        corrected_words.append(correction.split(' ')[0])\n",
        "    corrected_words.append(correction.split(' ')[1])\n",
        "    return corrected_words"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_nvbRGppMyn"
      },
      "source": [
        "# 3.1 Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78t1asNtpP9f",
        "outputId": "1bee2c72-2db3-4b84-dc32-8b2186ed11a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(correct_tokenized_text(twitter_tokenize))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', 't', 'cdnpoli', 'lpc', 'cpcldr', 't', 't', 't', 'https', 'immigration', 'integration', 'canada', 'https', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'Uk', 'economy', 't', 'Same', 'as', 'Australia', 't', 'Canada', 't', 'https', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', 't', 'https', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', 't', 'Xkofy', 'https', 'Canada', 'Immigration', 'Minister', 'to', 't', 't', 't', 'Substantially', 'Increase', 'Immigration', 'Numbers', 'https', 'https', 'M', 't', 't', 'me', 'les', 'usa', 'pays', 'as', \"d'immigration\", 'par', 'excellence', 'Contr', 't', 't', 'Le', 'Rigoureusement', \"l'immigration\", 'et', 'acc', 't', 't', 's', 't', 't', 'la', 'greencard', 't', 't', 't', 't', 'https', 'what', 'changes', 'should', 'be', 'made', 'to', \"Canada's\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', 't', 'L', 't', 't', 'immigration', 'irr', 't', 't', 'guli', 't', 't', 're', 'au', 'Canada', 'd', 't', 't', 'cortiqu', 't', 't', 'e', 'en', '5', 'questions', 'https', \"L'immigration\", 'irr', 't', 't', 'guli', 't', 't', 're', 'au', 'Canada', 'd', 't', 't', 'cortiqu', 't', 't', 'e', 'en', '5', 'questions', 'https', 'https', 'https', 'https', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', 't', 't', 't', 't', 't', 't', 't', 't', 'From', 'my', 'view', 't', 'immigration', 'immigration', 'out', 'of', 'C', 't', 't', 't', 'https', 'Dan', 'Murray', 'of', 't', 't', 'Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear', 'liar', 'racism', 'canada', 'cdnpoli', 'hatecrime', 't', 't', 't', 'https', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', 't', 't', 'son', 'besoin', 'de', 'main', \"d'\", 't', 't', 't', 'uvre', 'https', 'L', 't', 't', 'immigration', 'irr', 't', 't', 'guli', 't', 't', 're', 'au', 'canada', 'd', 't', 't', 'cortiqu', 't', 't', 'e', 'en', '5', 't', 't', 'questions', 'https', \"I've\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'Us', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', 'fasttraffic', 't', 'sitetraffic', 't', 'website', 't', 'traffic', 'https', 'Mr', 'Know', 'of', 'Canada', 'Immigration', 'https', 'Move', 'to', 'Canada', 'Oh', 't', 'immigration', 'rules', 't', 'you', \"can't\", '...', 'https', 'onthisday', 'Annette', 'Toft', 'becomes', \"Canada's\", '2', 'millionth', 'immigrant', 'since', '1945', 't', 'Do', 'you', 'know', 'your', \"family's\", 'immigration', 'st', 't', 't', 't', 'https', 't', 'profiles', \"Canada's\", 'open', 'immigration', 'policies', 't', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', 't', 't', 't', 't', 'https', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', 't', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https', 'Immigration', 'for', 'canada', 'without', 'india', 't', 'an', 'compassionate', 'handle', 't', 'deyfy', 't', 'jamaican', 'immigrants', 'canada', 'https', 'statistics', 'immigration', 't', 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', 't', '262m', 'over', 'a', 'decade', 'https', 'Are', 'people', 'still', 'moving', 'to', 'canada', 't', 't', 't', 'Oh', \"that's\", 'right', 't', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's\", 't', 't', 't', 'https', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', 't', 'B', 't', 'C', 't', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https', 'I', 'added', 'a', 'video', 'to', 'a', 'playlist', 'https', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa', 'To', 'Canada', 'https', 'https', 'L', 't', 't', 'immigration', 'irr', 't', 't', 'guli', 't', 't', 're', 'au', 'Canada', 'd', 't', 't', 'cortiqu', 't', 't', 'e', 'en', '5', 't', 't', 'questions', 'https', 'Hes', 'the', 'Pos', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', 't', 'among', 'other', 'globalist', 'policies', 't', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', 't', '2016', 't', 'Thoughts', 't', 'visa', 'immigration', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L3IGGKNtsFi"
      },
      "source": [
        "# 3.2 Student Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPtUW2WBt7GF",
        "outputId": "7f7a1e33-2bfb-4843-f096-f1147c129263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(correct_tokenized_text(student_feedback))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', 't', 'Lectures', 'are', 'understandable', 't', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self', 'also', 't', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', 't', 'to', 'Good', 't', 't', 't', 'br', 't', 't', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', 't', 't', '039', 't', 's', 'better', 'for', 'us', 't', 't', 'br', 't', 't', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', 't', 't', 'br', 't', 't', 't', 'br', 't', 't', 'Thanks', 't', 't', 't', 't', 'br', 't', 't', 'to', 'The', 'lectures', 'are', 'good', 'a', 'bit', 'speed', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'one', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame', \"d'\", 't', 'br', 't', 't', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', 't', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', 't', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', 't', 't', 'br', 't', 't', \"d'\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', 't', 'It', 'was', 'easy', 'to', 'understand', 't', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', 't', 'Lectures', 'were', 'good', 't', 'understandable', 't', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'Oop', 't', 'Motivated', 'to', 'well', 't', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', 't', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', 't', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', 't', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', 't', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples', 'were', 'interesting', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', 't', 'I', 'satisfy', 'about', 'first', '7', 'lectures', 't', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', 't', 'lectuers', 'are', 'very', 'good', 't', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', 't', 'very', 'helpfull', 't', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'Oop', 'and', 'its', 'concepts', 't', 'to', 'lecture', 'slides', 't', 'explanations', 'were', 'very', 'clear', 't', 't', 'br', 't', 't', 'it', 't', 't', '039', 't', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', 't', 't', 'br', 't', 't', 'sometimes', 't', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', 't', 't', 'br', 't', 't', 'overall', 'very', 'good', 't', 't', 't', 't', 'br', 't', 't', \"d'\", 'The', 'lectures', 'were', 'good', 'and', 'clear', 't', 'And', 'they', 'weren', 't', 't', '039', 't', 't', 'too', 'fast', 't', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', 't', 't', '039', 't', 't', 'know', 'java', 'before', 't', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'class', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', 't', 't', 'thankyou']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqaZJyMEuIzj"
      },
      "source": [
        "# 3.3 Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qplF4gTTuCzc",
        "outputId": "0f8433e7-c9c0-4ae4-8ffb-378674e01692",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(correct_tokenized_text(research_data))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi', 'learning', 't', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task', 'features', 't', 'However', 't', 'in', 'most', 'existing', 'approaches', 't', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 't', 'In', 'this', 'paper', 't', 'we', 'propose', 'an', 'adversarial', 'multi', 'learning', 'framework', 't', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', 't', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', 't', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', 't', 'Besides', 't', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', 't', 'Multi', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 't', 'Recently', 't', 'neural', 'models', 'for', 'multi', 'learning', 'have', 'become', 'very', 'popular', 't', 'ranging', 'from', 'computer', 'vision', 't', 'Misra', 'et', 'al', 't', '2016', 't', 'Zhang', 'et', 'al', 't', '2014', 't', 'to', 'natural', 'language', 'processing', 't', 'Collobert', 'andweston', 't', '2008', 't', 'Luong', 'et', 'al', 't', '2015', 't', 't', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', 't', 'However', 't', 'most', 'existing', 'work', 'on', 'multi', 'learning', 't', 'Liu', 'et', 'al', 't', '2016c', 't', 'b', 't', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 't', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', 't', 'As', 'shown', 'in', 'Figure', '1', 't', 'a', 't', 't', 'the', 'general', 'shared', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', 't', 'one', 'is', 'used', 'to', 'store', 'task', 'features', 't', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', 't', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task', 'features', 't', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 't', 'suffering', 'from', 'feature', 'redundancy', 't', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', 't', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', 't', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', 't', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 't', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', 't', 'The', 'word', 'infantile', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 't', 'However', 't', 'the', 'general', 'shared', 'model', 'could', 'place', 'the', 'task', 'word', 'infantile', 'in', 'a', 'shared', 'space', 't', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', 't', 'Additionally', 't', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', 't', 'To', 'address', 'this', 'problem', 't', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi', 'framework', 't', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints', 't', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', 't']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpZoruuRuuoQ"
      },
      "source": [
        "# 4. Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BFfH3VQuxCh"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "def stem_tokens(tokenized_text):\n",
        "    stemmed_text = []\n",
        "    for word in tokenized_text:\n",
        "        stemmed_text.append(stemmer.stem(word))\n",
        "    print(tokenized_text[0:10])\n",
        "    print(stemmed_text[0:10])\n",
        "    return stemmed_text"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tiz0UrK5vhkT"
      },
      "source": [
        "# 4.1 Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV95h1-Uvd-T",
        "outputId": "c47557bc-7897-446b-9e38-b71adb6d0c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = stem_tokens(twitter_tokenize)\n",
        "write_to_file(output, 'stemmer/{}_output.txt'.format('twitter'))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
            "['remind', 'me', 'of', 'liber', 'immigr', 'fraudster', 'monsef', 'avoid', 'deport', 'from']\n",
            "Written to /content/output.txt/stemmer/twitter_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3FKVKSRwDvz"
      },
      "source": [
        "# 4.2 Student Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cF6nhMBwHYm",
        "outputId": "8df7e0d8-bdef-4a01-c068-8ee7a410efd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = stem_tokens(student_feedback)\n",
        "write_to_file(output, 'stemmer/{}_output.txt'.format('feedback'))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
            "['honestli', 'last', 'seven', 'lectur', 'are', 'good', '.', 'lectur', 'are', 'understand']\n",
            "Written to /content/output.txt/stemmer/feedback_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aGY-gh9wTn2"
      },
      "source": [
        "# 4.3 Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srbeJBxTwd66",
        "outputId": "ac38371c-580c-4a15-b8b0-62d8dd375906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = stem_tokens(research_data)\n",
        "write_to_file(output, 'stemmer/{}_output.txt'.format('research'))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task']\n",
            "['neural', 'network', 'model', 'have', 'shown', 'their', 'promis', 'opportun', 'for', 'multi-task']\n",
            "Written to /content/output.txt/stemmer/research_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC5tExLfwp6g"
      },
      "source": [
        "# 5. Lemmetization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgvgrTIww2S2"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "def lemmatize_tokens(tokenized_text):\n",
        "    lemmatized_text = []\n",
        "    for word in tokenized_text:\n",
        "        lemmatized_text.append(lemmatizer.lemmatize(word))\n",
        "    print(tokenized_text[0:10])    \n",
        "    print(lemmatized_text[0:10])\n",
        "    return lemmatized_text"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd8--ZlAwvec"
      },
      "source": [
        "# 5.1 Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWdTBg7ywtXB",
        "outputId": "a9c69bf0-4720-4d05-a256-1cb8b103661b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = lemmatize_tokens(twitter_tokenize)\n",
        "write_to_file(output, 'lemmatize/{}_output.txt'.format('twitter'))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
            "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from']\n",
            "Written to /content/output.txt/lemmatize/twitter_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRnu9AhoxZRn"
      },
      "source": [
        "# 5.2 Student Feedback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlRGJCz5xci3",
        "outputId": "be0da607-3db9-45ed-990a-8f04dccd50ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = lemmatize_tokens(student_feedback)\n",
        "write_to_file(output, 'lemmatize/{}_output.txt'.format('feedback'))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
            "['Honestly', 'last', 'seven', 'lecture', 'are', 'good', '.', 'Lectures', 'are', 'understandable']\n",
            "Written to /content/output.txt/lemmatize/feedback_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgFakcJ5xo9g"
      },
      "source": [
        "# 5.3 Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKnn4V7Rxtx9",
        "outputId": "1137904a-d616-41fd-e484-f9822aa89d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = lemmatize_tokens(research_data)\n",
        "write_to_file(output, 'lemmatize/{}_output.txt'.format('research'))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task']\n",
            "['Neural', 'network', 'model', 'have', 'shown', 'their', 'promising', 'opportunity', 'for', 'multi-task']\n",
            "Written to /content/output.txt/lemmatize/research_output.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}